作为机器学习中的一大类模型，树模型一直以来都颇受学界和业界的重视。
## 决策树
直观理解：
- 可以将决策树看作是一组if-then规则的集合
- 决策树的每个节点对应着对一个特征的考察，每条边对应该特征的一个取值（或者满足的一个条件）。
- 每个节点还对应一组数据，从根节点到叶子节点的路径，代表了叶子节点对应的那组数据所满足的规则。
- 决策树还代表了对于空间坐标的一组划分，每个特征对应一维（坐标轴），生成决策树的过程就是不断用垂直于坐标轴的直线（平面）对空间进行划分，也可以是认为是对一个函数的定义域进行划分，划分后每个小空间区域内的所有自变量对应一个y，从这个角度看，决策树也可以看作分段函数。

### ID3
处理的数据：
- 三个维度：数据集有n个样本，每个数据有k个特征，每个特征有m种取值
- ID3算法一般用来处理类别型数据（特征取值为离散数据），比如性别特征取值为男和女。

模型：
- 决策树就是一个分段函数，只不过该函数的自变量取值是离散的。

策略：
- 如果只考虑经验风险，决策树追求的目标是，生成一个分段函数，让分段函数能精确的匹配每个样本数据。
- 如果再考虑结构风险，可能会对决策树的树深、叶子节点的数量进行限制，那么这时候就是在一定约束下，使得分段函数能尽量多的匹配样本点。

算法：
- 决策树的生成，采用启发式的贪心算法，生成规则集合（空间切分顺序）。
- 从k个特征中，选择一个特征，按照该特征的m个不同取值，将样本数据进行划分为m组，计算划分前和划分后的信息熵增益。考察k个特征产生的信息熵增益，选择产生信息增益最大的特征作为规则判断的特征。
  - 信息熵
    > 若离散随机变量X的概率分布为：$P(X=x_i) = p_i$, 则随机变量X的熵定义为：$H(X)=-\sum_{i=1}^{n}p_ilogp_i$ 在信息论里面，熵是一种表示随机变量不确定性的度量方式。

  - 条件熵
    > 当给定随机变量X的条件的条件下随机变量Y的熵可定义为条件熵$H(Y|X)$ : $H(Y|X) = -\sum_{i=1}^{n}p_iH(Y|X = x_i)$，其中$P(X=x_i)=p_i$为离散随机变量X的概率分布。

  - 信息增益
    > 假设数据集D的信息熵为H(D)，给定特征A之后的条件熵为$H(D|A)$, 则特征A对于数据集的信息增益$g(D, A)$可表示为：$g(D, A)=H(D)-H(D|A)$ 信息熵增益是数据在得到特征X的信息时，使得类Y的信息不确定性减少的程度。
- 重复上述操作，直到样本的特征都一样，无法继续划分，或者样本都属于同一个类别。

### C4.5
- ID3算法的缺点
  - 倾向于选择特征值较多的特征，比如ID类的特征，造成模型容易过拟合

- C4.5算法对于ID3算法的改进
  - C4.5算法采用信息增益比作为特征选择的依据
    > 特征A对训练数据集D的信息增益比$g_R(D, A)$定义为其信息增益$g(D, A)$与训练数据D关于特征A的值的熵$H_A(D)$之比，即
    > $$g_R(D, A)=\frac{g(D, A)}{H_A(D)}$$
    > 其中，$H_A(D)=-\sum_{i=1}^{n}\frac{|D_i|}{|D|}log_2\frac{|D_i|}{|D|}$, n是特征A取值的个数。

### CART
CART(Classification and Regression Tree)即分类与回归树。与ID3和C4.5的主要区别如下：
- CART既可以用于分类任务，还可以完成回归分析。
- 使用CART算法生成决策树的过程中，每个节点有两个分支。
- CART算法采用基尼系数作为切分点选择的依据。

- 回归树的生成
  - 模型：一个回归树对应着输入空间（即特征空间）的一个划分，以及在划分的单元上的输出值（分段函数）。假设已将输入空间划分为M个单元$R_1, R_2, ..., R_m$,并且在每个单元$R_i$上有一个固定的输出值$c_i$，于是回归树模型可以表示为 $f(x)=\sum_{i=1}^mc_iI(x\in R_i)$， $I(x)$为指示函数，当条件为真时输出1，条件为假时输出0。
  - 策略：当输入空间的划分确定时，对于任一区间$R_i$可以采用平方误差$\sum_{x_i\in R_i}(y_i-f(x_i))^2$来表示回归树对于训练数据的预测误差，用平方误差最小的准则求解每个单元上的最优输出值。易知，单元$R_i$上的$c_i$的最优值是$R_i$上的所有输入实例$x_i$对应的输出$y_i$的均值，即$\hat{c_i}=ave(y_j|x_j\in R_i)$。然后，还需要选择最优的空间划分，做法是选择第j个特征$x^{(j)}$和它的取值s，作为切分变量和切分点，并定义两个区域$R_1(j,s)=\{x|x^(j)\leq s\}$和$R_2(j,s)=\{x|x^{(j)} \gt s\}$，然后寻找最优切分变量j和最优切分点s。具体地，求解
  $$\mathop{min}\limits_{j,s}[\mathop{min}\limits_{c_1}\sum_{x_i\in R_1(j,s)}(y_i-c_1)^2+\mathop{min}\limits_{c_1}\sum_{x_i\in R_2(j,s)}(y_i-c_2)^2]$$
  - 算法：采用贪心的策略，遍历每个特征、每个特征取值，尝试划分空间之后，计算损失，然后选择其中最优的一个特征以及一个具体地特征取值作为当前划分点，得到多个划分区间，然后对每个划分区间按照前面的方式迭代，直到满足一定条件结束（损失足够小或者区域中只有一个样本点）。
  
  
## 随机森林

## GBDT
GBDT(Gradient Boosting Decision Tree), 即梯度提升决策树，是一种集成学习模型。主要的集成学习方法有如下几种：
- Bagging，主要思想是投票平均。
- Boosting，主要思想是通过将单个弱学习器进行线性组合构成一个强学习器的。
- Stacking

GBDT属于Boosting类型的集成学习模型，其使用CART作为弱学习器（基模型），并且融入梯度下降对模型进行优化。

- 模型
  
  一个提升树模型可以描述为：
  $$\mathcal{f}_M(x)=\sum_{m=1}^MT(x;\Theta_m)$$
  $T(x;\Theta)$代表CART模型，也就是一棵回归树，可以表示为：$T(x;\Theta)=\sum_{j=1}^Jc_jI(x\in R_j)$，x为特征数据，$\Theta$为模型参数。这是一个加法模型，整个模型的输出由多棵决策树的输出组成。

- 策略
  
  GBDT既可以用来做分类任务也可以用来做回归任务，只需要在模型输出上接不同的损失函数即可。

- 算法
  
  提升树利用前向分步算法实现学习的优化过程。
  第0步、第m步和最终模型可表示为：
  $$f_0(x)=0$$
  $$f_m(x)=f_{m-1}(x)+T(x;\Theta_m), m=1,2,...,M$$
  $$f_M(x)=\sum_{m=1}^MT(x;\Theta_m)$$
  当给定第m-1步的模型下，求解：
  $$\hat{\Theta}_m=\mathcal{argmin}_{\Theta_m}\sum_{i=1}^NL(y_i, f_{m-1}(x_i)+T(x_i+T(x_i;\Theta_m)))$$
  当损失函数为平方损失时：
  $$L(y,f(x))=(y-f(x))^2$$
  相应的损失可推导为：
  $$L(y, f_{m-1}(x)+T(x;\Theta_m))$$
  $$=[y-f_{m-1}-T(x;\Theta_m)]^2$$
  $$=[r-T(x;\Theta_m)]^2$$
  其中，$r=y-f_{m-1}(x)$

> 提升树每一次迭代是在拟合一个残差函数。

但是在实际工作中并不是每一个损失函数都如平方损失那样容易优化，所以有学者提出近似梯度下降（最速下降法）的方法，使用使用损失函数的负梯度在当前模型的值作为回归提升中残差的近似值。即：
$$r_{mi}=-\bigg[\frac{\partial L(y_i, f(x_i))}{\partial f(x_i)} \bigg]_{f(x)=f_{m-1}(x)}$$
  
  
## AdaBoost

## XGBoost

## LightGBM

## CatBoost