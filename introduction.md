作为机器学习中的一大类模型，树模型一直以来都颇受学界和业界的重视。
## 决策树
直观理解：
- 可以将决策树看作是一组if-then规则的集合
- 决策树的每个节点对应着对一个特征的考察，每条边对应该特征的一个取值（或者满足的一个条件）。
- 每个节点还对应一组数据，从根节点到叶子节点的路径，代表了叶子节点对应的那组数据所满足的规则。
- 决策树还代表了对于空间坐标的一组划分，每个特征对应一维（坐标轴），生成决策树的过程就是不断用垂直于坐标轴的直线（平面）对空间进行划分，也可以是认为是对一个函数的定义域进行划分，划分后每个小空间区域内的所有自变量对应一个y，从这个角度看，决策树也可以看作分段函数。

### ID3
处理的数据：
- 三个维度：数据集有n个样本，每个数据有k个特征，每个特征有m种取值
- ID3算法一般用来处理类别型数据（特征取值为离散数据），比如性别特征取值为男和女。

模型：
- 决策树就是一个分段函数，只不过该函数的自变量取值是离散的。

策略：
- 如果只考虑经验风险，决策树追求的目标是，生成一个分段函数，让分段函数能精确的匹配每个样本数据。
- 如果再考虑结构风险，可能会对决策树的树深、叶子节点的数量进行限制，那么这时候就是在一定约束下，使得分段函数能尽量多的匹配样本点。

算法：
- 决策树的生成，采用启发式的贪心算法，生成规则集合（空间切分顺序）。
- 从k个特征中，选择一个特征，按照该特征的m个不同取值，将样本数据进行划分为m组，计算划分前和划分后的信息熵增益。考察k个特征产生的信息熵增益，选择产生信息增益最大的特征作为规则判断的特征。
  - 信息熵
    > 若离散随机变量X的概率分布为：$P(X=x_i) = p_i$, 则随机变量X的熵定义为：$H(X)=-\sum_{i=1}^{n}p_ilogp_i$

  - 条件熵
    > 当给定随机变量X的条件的条件下随机变量Y的熵可定义为条件熵$H(Y|X)$ : $H(Y|X) = -\sum_{i=1}^{n}p_iH(Y|X = x_i)$，其中$P(X=x_i)=p_i$为离散随机变量X的概率分布。
- 重复上述操作，直到样本的特征都一样，无法继续划分，或者样本都属于同一个类别。

### C4.5
### CART

## 随机森林

## GBDT

## AdaBoost

## XGBoost

## LightGBM

## CatBoost