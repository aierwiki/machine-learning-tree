作为机器学习中的一大类模型，树模型一直以来都颇受学界和业界的重视。
## 决策树
直观理解：
- 可以将决策树看作是一组if-then规则的集合
- 决策树的每个节点对应着对一个特征的考察，每条边对应该特征的一个取值（或者满足的一个条件）。
- 每个节点还对应一组数据，从根节点到叶子节点的路径，代表了叶子节点对应的那组数据所满足的规则。
- 决策树还代表了对于空间坐标的一组划分，每个特征对应一维（坐标轴），生成决策树的过程就是不断用垂直于坐标轴的直线（平面）对空间进行划分，也可以是认为是对一个函数的定义域进行划分，划分后每个小空间区域内的所有自变量对应一个y，从这个角度看，决策树也可以看作分段函数。

### ID3
处理的数据：
- 三个维度：数据集有n个样本，每个数据有k个特征，每个特征有m种取值
- ID3算法一般用来处理类别型数据（特征取值为离散数据），比如性别特征取值为男和女。

模型：
- 决策树就是一个分段函数，只不过该函数的自变量取值是离散的。

策略：
- 如果只考虑经验风险，决策树追求的目标是，生成一个分段函数，让分段函数能精确的匹配每个样本数据。
- 如果再考虑结构风险，可能会对决策树的树深、叶子节点的数量进行限制，那么这时候就是在一定约束下，使得分段函数能尽量多的匹配样本点。

算法：
- 决策树的生成，采用启发式的贪心算法，生成规则集合（空间切分顺序）。
- 从k个特征中，选择一个特征，按照该特征的m个不同取值，将样本数据进行划分为m组，计算划分前和划分后的信息熵增益。考察k个特征产生的信息熵增益，选择产生信息增益最大的特征作为规则判断的特征。
  - 信息熵
    > 若离散随机变量X的概率分布为：$P(X=x_i) = p_i$, 则随机变量X的熵定义为：$H(X)=-\sum_{i=1}^{n}p_ilogp_i$ 在信息论里面，熵是一种表示随机变量不确定性的度量方式。

  - 条件熵
    > 当给定随机变量X的条件的条件下随机变量Y的熵可定义为条件熵$H(Y|X)$ : $H(Y|X) = -\sum_{i=1}^{n}p_iH(Y|X = x_i)$，其中$P(X=x_i)=p_i$为离散随机变量X的概率分布。

  - 信息增益
    > 假设数据集D的信息熵为H(D)，给定特征A之后的条件熵为$H(D|A)$, 则特征A对于数据集的信息增益$g(D, A)$可表示为：$g(D, A)=H(D)-H(D|A)$ 信息熵增益是数据在得到特征X的信息时，使得类Y的信息不确定性减少的程度。
- 重复上述操作，直到样本的特征都一样，无法继续划分，或者样本都属于同一个类别。

### C4.5
- ID3算法的缺点
  - 倾向于选择特征值较多的特征，比如ID类的特征，造成模型容易过拟合

- C4.5算法对于ID3算法的改进
  - C4.5算法采用信息增益比作为特征选择的依据
    > 特征A对训练数据集D的信息增益比$g_R(D, A)$定义为其信息增益$g(D, A)$与训练数据D关于特征A的值的熵$H_A(D)$之比，即
    > $$g_R(D, A)=\frac{g(D, A)}{H_A(D)}$$
    > 其中，$H_A(D)=-\sum_{i=1}^{n}\frac{|D_i|}{|D|}log_2\frac{|D_i|}{|D|}$, n是特征A取值的个数。

### CART
CART(Classification and Regression Tree)即分类与回归树。与ID3和C4.5的主要区别如下：
- CART既可以用于分类任务，还可以完成回归分析。
- 使用CART算法生成决策树的过程中，每个节点有两个分支。
- CART算法采用基尼系数作为切分点选择的依据。

- 回归树的生成
  - 模型：一个回归树对应着输入空间（即特征空间）的一个划分，以及在划分的单元上的输出值（分段函数）。假设已将输入空间划分为M个单元$R_1, R_2, ..., R_m$,并且在每个单元$R_i$上有一个固定的输出值$c_i$，于是回归树模型可以表示为 $f(x)=\sum_{i=1}^mc_iI(x\in R_i)$， $I(x)$为指示函数，当条件为真时输出1，条件为假时输出0。
  - 策略：当输入空间的划分确定时，对于任一区间$R_i$可以采用平方误差$\sum_{x_i\in R_i}(y_i-f(x_i))^2$来表示回归树对于训练数据的预测误差，用平方误差最小的准则求解每个单元上的最优输出值。易知，单元$R_i$上的$c_i$的最优值是$R_i$上的所有输入实例$x_i$对应的输出$y_i$的均值，即$\hat{c_i}=ave(y_j|x_j\in R_i)$。然后，还需要选择最优的空间划分，做法是选择第j个特征$x^{(j)}$和它的取值s，作为切分变量和切分点，并定义两个区域$R_1(j,s)=\{x|x^(j)\leq s\}$和$R_2(j,s)=\{x|x^{(j)} \gt s\}$，然后寻找最优切分变量j和最优切分点s。具体地，求解
  $$\mathop{min}\limits_{j,s}[\mathop{min}\limits_{c_1}\sum_{x_i\in R_1(j,s)}(y_i-c_1)^2+\mathop{min}\limits_{c_1}\sum_{x_i\in R_2(j,s)}(y_i-c_2)^2]$$
  - 算法：采用贪心的策略，遍历每个特征、每个特征取值，尝试划分空间之后，计算损失，然后选择其中最优的一个特征以及一个具体地特征取值作为当前划分点，得到多个划分区间，然后对每个划分区间按照前面的方式迭代，直到满足一定条件结束（损失足够小或者区域中只有一个样本点）。
  
  
## 随机森林

## GBDT
GBDT(Gradient Boosting Decision Tree), 即梯度提升决策树，是一种集成学习模型。主要的集成学习方法有如下几种：
- Bagging，主要思想是投票平均。
- Boosting，主要思想是通过将单个弱学习器进行线性组合构成一个强学习器的。
- Stacking

GBDT属于Boosting类型的集成学习模型，其使用CART作为弱学习器（基模型），并且融入梯度下降对模型进行优化。

- 模型
  
  一个提升树模型可以描述为：
  $$\mathcal{f}_M(x)=\sum_{m=1}^MT(x;\Theta_m)$$
  $T(x;\Theta)$代表CART模型，也就是一棵回归树，可以表示为：$T(x;\Theta)=\sum_{j=1}^Jc_jI(x\in R_j)$，x为特征数据，$\Theta$为模型参数。这是一个加法模型，整个模型的输出由多棵决策树的输出组成。

- 策略
  
  GBDT既可以用来做分类任务也可以用来做回归任务，只需要在模型输出上接不同的损失函数即可。

- 算法
  
  提升树利用前向分步算法实现学习的优化过程。
  第0步、第m步和最终模型可表示为：
  $$f_0(x)=0$$
  $$f_m(x)=f_{m-1}(x)+T(x;\Theta_m), m=1,2,...,M$$
  $$f_M(x)=\sum_{m=1}^MT(x;\Theta_m)$$
  当给定第m-1步的模型下，求解：
  $$\hat{\Theta}_m=\mathcal{argmin}_{\Theta_m}\sum_{i=1}^NL(y_i, f_{m-1}(x_i)+T(x_i+T(x_i;\Theta_m)))$$
  当损失函数为平方损失时：
  $$L(y,f(x))=(y-f(x))^2$$
  相应的损失可推导为：
  $$L(y, f_{m-1}(x)+T(x;\Theta_m))$$
  $$=[y-f_{m-1}-T(x;\Theta_m)]^2$$
  $$=[r-T(x;\Theta_m)]^2$$
  其中，$r=y-f_{m-1}(x)$

> 提升树每一次迭代是在拟合一个残差函数。

但是在实际工作中并不是每一个损失函数都如平方损失那样容易优化，所以有学者提出近似梯度下降（最速下降法）的方法，使用使用损失函数的负梯度在当前模型的值作为回归提升中残差的近似值。即：
$$r_{mi}=-\bigg[\frac{\partial L(y_i, f(x_i))}{\partial f(x_i)} \bigg]_{f(x)=f_{m-1}(x)}$$
  
  
## AdaBoost


## XGBoost
XGBoost（eXtreme Gradient Boosting），即极端梯度提升树，也属于boosting集成学习算法。它与GBDT的主要区别在于：
- GBDT的前向分步算法中，每一步基学习器所拟合的是损失函数在当前步的负梯度。但是XGBoost每一步基学习器所拟合的是损失函数在当前步进行二阶泰勒展开之后，然后求出的需要拟合的值。
- GBDT的损失函数中没有考虑结构风险，XGBoost的损失函数加入了对模型复杂度的约束。
  
XGBoost
- 模型

XGBoost也是有多个基模型组成的一个加法模型
$$\hat{y}=\sum_{k=1}^Kf_k(x_i)$$
K为基模型（CART）的数量，$f_k$为基模型，$x_i$为特征输入。

- 策略

XGBoost可以做回归任务也可以做分类任务，只需要在模型输出上接不同的损失函数。XGBoost的损失函数为：
$$Obj = \sum_{i=1}^nl(y_i, \hat{y_i}) + \sum_{i=1}^t\Omega(f_i)$$
其中$\hat{y_i}$为模型的输出，$\sum_{i=1}^t\Omega(f_i)$为损失函数的正则化项，表示全部t棵树的复杂度之和，旨在防止模型过拟合。

- 算法

XGBoost来自于GBDT，同样适用于前向分步算法，以第t步的模型为例，模型对第i个样本的预测值为：
$$\hat{y_i}^{(t)}=\hat{y_i}^{t-1}+f_t(x_i)$$
其中，$\hat{y_i}^{(t-1)}$是由第t-1步的模型给出的预测值，作为一个已知常数存在，$f_t(x_i)$是第t步树模型的预测值。所以目标函数可以改写为：
$$Obj^{(t)}=\sum_{i=1}^nl(y_i, \hat{y_i}^{(t)})+\sum_{i=1}^t\Omega(f_i)$$
$$=\sum_{i=1}^nl(y_i, \hat{y_i}^{(t-1)}+f_t(x_i))+\sum_{i=1}^t\Omega(f_i)$$
$$=\sum_{i=1}^nl(y_i, \hat{y_i}^{(t-1)}+f_t(x_i))+\sum_{i=1}^{t-1}\Omega(f_i)+\Omega(f_i)$$
$$=\sum_{i=1}^{n}l(y_i, \hat{y_i}^{(t-1)}+f_t(x_i)) + \Omega(f_t) + constant$$

由于前t-1棵树的结构已经确定，因此前t-1棵树的复杂度之和可以表示为常数。

经验损失部分可以使用二阶泰勒展开如下：
$$l(y_i, \hat{y_i}^{(t-1)}+f_t(x_i))=l(y_i, \hat{y_i}^{(t-1)})+g_if_t(x_i)+\frac{1}{2}h_if_t^2(x_i)$$
其中，$g_i$为损失函数的一阶导，$h_i$为损失函数的二阶导.注意：此处的导数是对$\hat{y_i}^{(t-1)}$求导。如果使用自定义损失函数，要求其二阶可导。

将二阶泰勒展开带入损失函数中，可得到损失函数的近似表达式：
$$Obj^{(t)}\approxeq \sum_{i=1}^n[l(y_i, \hat{y_i}^{t-1})+g_if_t(x_i)+\frac{1}{2}h_if_t^2(x_i)]+\Omega(f_t)+constant$$
对上式去除相关常数项，简化后的损失函数为：
$$Obj^{(t)}\approxeq \sum_{i=1}^n[g_if_t(x_i)+\frac{1}{2}h_if_t^2(x_i)]+\Omega(f_t)$$
只需要求出每一步损失函数的一阶导和二阶导的值，然后最优化目标函数，就可以得到每一步的$f(x)$，可以理解为要当前步要优化的目标，使用一棵决策树去进行优化。

一棵树的数学表达式为:
$$f_t(x)=w_{q(x)}$$
该表达式包括两部分：
- 叶子节点的权重向量$\bold{w}$
- 样本实例到叶子节点的映射关系$\bold{q}$, $q(x)$表示样本实例x属于哪个叶子节点。
  
模型复杂度$\Omega$可由单棵树的叶子结点数$T$和叶子权重$w$所决定，数学表达式如下：
$$\Omega(f_t)=\gamma T+\frac{1}{2}\lambda\sum_{j=1}^Tw_j^2$$
对所有的叶子节点进行重新归组，将属于第j个叶子节点的所有样本$x_i$划入到一个叶子结点的样本集合中，即: $I_j=\{i|q(x_i)=j\}$，从而XGBoost的目标函数可以改写为：
$$Obj^t\approxeq \sum_{i=1}^n\Big[g_if_t(x_i)+\frac{1}{2}h_if_t^2(x_i)\Big]+\Omega(f_t)$$
$$=\sum_{i=1}^n\Big[g_iw_{q(x_i)}+\frac{1}{2}h_iw_{(x_i)}^2\Big]+\gamma T+\frac{1}{2}\lambda \sum_{j=1}^Tw_j^2$$
$$=\sum_{j=1}^T\Big[(\sum_{i\in I_j}g_i)w_j+\frac{1}{2}(\sum_{i \in I_j}h_i+\lambda)w^2\Big]+\gamma T$$
定义
$$G_j=\sum_{i \in I_j}g_i, H_j=\sum_{i\in I_j}h_i$$
- $G_j$：叶子结点j所包含样本的一阶偏导数累加之和，是一个常量。
- $H_j$: 叶子结点j所包含样本的二阶导偏导数累加之和，是一个常量。
  
将$G_j$和$H_j$带入前述XGBoost损失函数，可得最终的损失函数表示为：
$$Obj^{(t)}=\sum_{j=1}^T\Big[G_jw_j+\frac{1}{2}(H_j+\lambda)w_j^2\Big]+\gamma T$$

上述目标函数中，每个叶子结点对应一个目标函数:$G_jw_j+\frac{1}{2}(H_j+\lambda)w_j^2$，该式就是一个只包含一个变量叶子结点权重$w_j$的一元二次函数，可根据最值公式求其最值点。当每个叶子结点都达到最优值时，整个损失函数也相应的达到最优。
当树结构固定的情况下，对上式求导，并令其为0，可得最优点和最优值为：
$$w_j^*=-\frac{G_j}{H_j+\lambda}$$
$$Obj = -\frac{1}{2}\sum_{j=1}^T\frac{G_j^2}{H_j+\lambda}+\gamma T$$
XGBoost的结点分裂方式跟CART树的结点分裂方式本质上并没有太大区别，但是信息增益的计算方式有所不同。
假设模型在某一结点完成特征分裂，分裂前的目标函数可以写为：
$$Obj_1=-\frac{1}{2}\Big[\frac{(G_L+G_R)^2}{H_L+H_R+\lambda}\Big]+\gamma$$
分裂后的目标函数为：
$$Obj_2=-\frac{1}{2}\Big[\frac{G_L^2}{H_L+\lambda}+\frac{G_R^2}{H_R+\lambda}\Big]+2\gamma$$
则对于目标函数来说，分裂后的收益为：
$$Gain=\frac{1}{2}\Big[\frac{G_L^2}{H_L+\lambda}+\frac{G_R^2}{H_R+\lambda}-\frac{(G_L+G_R)^2}{H_L+H_R+\lambda}\Big]-\lambda$$
如果增益Gain>0，即分裂为两个叶子结点后，目标函数下降了，则考虑此次分裂的结果。实际处理时需要遍历所有特征寻找最佳分裂特征。

## LightGBM

## CatBoost